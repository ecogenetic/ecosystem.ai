---
title: Q-Learning
description: Q-Learning
---

# Q-Learning

The Q-learning algorithm uses the Q-learning reinforcement learning framework with actions being presenting offers to clients and states being whether the client accepted the offer. The algorithm works at a customer level, scoring is done per customer. Segment level implementations are planned for future releases.

## Algorithm

The Q-learning algorithm represents offers as the set of actions $A$ and the states $S$ represent take up of an offer. The algorithm then uses a Q-table to score and rank offers using the Q value as this expresses the expected utility of the offer.

The Q-table is updated using a policy function, which calculates the next action, and a reward function, which calculates the reward for a given action.

## Parameters

- **Processing Window**: Restricts the data used when the model updates based on a time period from the present going back a specified in milliseconds.
- **Historical Count**: Restricts the data used when the model updates based on a count of interactions. The count used is per offer and segment.
- **Decay Parameter**: Used to treat repeated interactions from the same customer differently from one off interactions from individual customers. The weight of each interaction from a customer is reduced by a factor of one over the decay parameter, i.e. the latest interaction has a weight of one, the interaction before that has a weight of one over the decay parameter and the interaction before that has a weight of one over the decay parameter squared
- **Max interactions**: Used to treat repeated interactions from the same customer differently from one off interactions from individual customers. Restricts the number of interactions from an individual customer that will be used when updating the model - the latest interactions will be used.
- **Success Reward**: The size of the increment to the alpha parameter of the beta distributions used in the Thompson Sampling when an interaction is successful. This impacts the rate of convergence.
- **Fail Reward**: The size of the increment to the beta parameter of the beta distributions used in the Thompson Sampling when an interaction is not successful. This impacts the rate of convergence.
- **Prior Success Reward**: The size of the increment to the alpha parameter of the beta distributions used in the Thompson Sampling when an interaction is successful in the historical data. Used when historical data is used to train the algorithm before deployment.
- **Prior Fail Reward**: The size of the increment to the beta parameter of the beta distributions used in the Thompson Sampling when an interaction is not successful in the historical data. Used when historical data is used to train the algorithm before deployment.
- **Test options across segments**: If there are options that are configured to only be available for specific values of the contextual variables, electing to test options across segments will occasionally predict those options for contextual variable values where they are not available

