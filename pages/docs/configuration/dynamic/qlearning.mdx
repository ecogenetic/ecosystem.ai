---
title: Q-Learning
description: Q-Learning
---

# Q-Learning

The Q-learning algorithm uses the Q-learning reinforcement learning framework with actions being presenting offers to clients and states being whether the client accepted the offer. The algorithm works at a customer level, scoring is done per customer. Segment level implementations are planned for future releases.

## Algorithm

The Q-learning algorithm represents offers as the set of actions $\mathcal{A}$ and the states $\mathcal{S}$ represent take up of an offer. The algorithm then uses a Q-table to score and rank offers using the Q value as this expresses the expected utility of the offer.

The Q-table is updated using a policy function, which calculates the next action, and a reward function, which calculates the reward for a given action. The Q-table is calculated for each customer as a recommendation is made. The first row of the table is generated by selecting and initial state and action at random, calculating the reward using the reward function and then calculating Q using $Q=\alpha (R + \gamma R_{max})$, where $alpha$ is the learning rate, $R$ is the reward returned by the reward function, \gamma is the discount factor and $R_{max}$ is the maximum reward. The Q-table is then updated using the Q value and the reward. The subsequent rows of the Q-table are generated by selecting an action usinf the policy function, calculating the reward using the reward function and then calculating Q using the full Bellman equation. The Q-table is then updated using the Q value and the reward.

The policy function is a function that selects the next action based on the current state of the Q-table. The policy function used by the algorithm is $\epsilon$-greedy. The action with the highest Q value is selected with probability $1-\epsilon$ and a random action is selected with probability $\epsilon$.

The reward function is a java plugin that calculates the reward for a given action. The reward function should be calibrated be for specific use case. Below is an illustrative reward function which uses cop car values for offers and historic purchase information to caluclate the reward. This java class should be located in the `src/main/java/com/ecosystem/algorithm/qlearn` directory.

```java
package com.ecosystem.algorithm.qlearn;

import com.ecosystem.plugin.PluginLoaderSuper;
import com.ecosystem.utils.log.LogManager;
import com.ecosystem.utils.log.Logger;
import org.json.JSONObject;

import java.util.List;
import java.util.ArrayList;
import java.util.Arrays;

public class QLearnRewardPlugin extends PluginLoaderSuper {

    private static final Logger LOGGER = LogManager.getLogger(QLearnRewardPlugin.class.getName());

    /**
     *
     * @param actions
     * @param action
     * @param state
     * @param offers
     * @param category
     * @return
     */
    public double reward(List<String> actions, String action, int state, List<String> offers, int category, JSONObject params) {

        double reward = 0.0;

        try {
            if (params.has("preloadCorpora")) {
                if (params.getJSONObject("preloadCorpora").has("copcar_lookup")) {
                    JSONObject copcar_lookup  = params.getJSONObject("preloadCorpora").getJSONObject("copcar_lookup");
                    JSONObject copcar_fields = copcar_lookup.getJSONObject("copcar");
                    String copcar_fields_string = String.valueOf(copcar_fields.get("value"));
                    ArrayList<String> copcarList = new ArrayList<>(Arrays.asList(copcar_fields_string.split(",")));
                    for (String copcar_key : copcarList) {
                        double  copcar_value = Double.valueOf(String.valueOf(params.getJSONObject("featuresObj").get(copcar_key)));
                        copcar_list.add(copcar_value);
                    }
                }
            }

            int list_size = copcar_list.size();
            if (list_size%2 == 1) {
                copcar_median = copcar_list.get(((list_size+1)/2)-1);
            } else {
                if (list_size > 0)
                    copcar_median = copcar_list.get((list_size/2)-1) + copcar_list.get(list_size/2);
                else
                    copcar_median = 0;
            }

            double previous_day_purchase = 0;
            if (params.getJSONObject("featuresObj").has("previous_day_purchase"))
                previous_day_purchase = Double.valueOf(String.valueOf(params.getJSONObject("featuresObj").get("previous_day_purchase")));
            else
                LOGGER.info("previous_day_purchase not found in featuresObj");

            if (previous_day_purchase > 0) {
                state = 1;
            } else {
                state = 0;
            }

            double cost = Double.parseDouble(offers.get(actions.indexOf(action)));

            if (cost > copcar_median && state == 1) {
                reward = 10;
            } else if (cost == copcar_median && state == 1) {
                reward = 8;
            } else if (cost < copcar_median && state == 1) {
                reward = 0;
            } else if (cost > copcar_median && state == 0) {
                reward = -10;
            } else if (cost == copcar_median && state == 0) {
                reward = -10;
            } else if (cost < copcar_median && state == 0) {
                reward = -10;
            } else {
                reward = 0;
            }

        } catch (Exception e) {
            e.printStackTrace();
            LOGGER.error(e.getMessage());
        }

        return reward;

    }
}
```

## Parameters

- **Processing Window**: Restricts the data used when the model updates based on a time period from the present going back a specified in milliseconds.
- **Historical Count**: Restricts the data used when the model updates based on a count of interactions. This is an overall interaction count rather than a count per offer and segment as used in the Ecosystem Rewards algorithm.
- **Learning Rate**: The learning rate $\alpha$ used in the Bellman equation when calculating Q. 
- **Discount Factor**: The discount factor $\gamma$ used in the Bellman equation when calculating Q.
- **Maximum Reward**: The maximum reward $R_{max}$ used in the Bellman equation when calculating Q.
- **Random Action**: The value for $\epsilon$ used in the $\epsilon$-greedy policy function. 
